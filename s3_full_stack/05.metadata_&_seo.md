
# **5. SEO & Metadata in Next.js**

## **Why SEO and Metadata Matter**

Imagine you built the best **Swiggy clone** — beautiful UI, lightning-fast, all restaurant menus updated daily.
But when people search *"best biryani in Mumbai"* on Google, your site **doesn’t appear anywhere**.

It’s like opening the most amazing restaurant in a hidden alley **with no signboard**.
No matter how good your food is, no one finds it.

This is where **SEO (Search Engine Optimization)** and **metadata** come in.
Metadata is the *behind-the-scenes information* that tells **Google, Bing, social media, and browsers**:

* What your page is about
* Who it’s for
* How to display it attractively

If you’ve ever **shared a Zomato restaurant link** on WhatsApp and seen a nice preview with the restaurant’s name, photo, and ratings — that’s **metadata + OpenGraph** in action.

And in **Next.js**, we can manage all of this cleanly using tools like:

* `next/head` — Page titles, descriptions, OpenGraph tags
* **robots.txt** — Tell search engines what to index
* **Sitemap.xml** — Show them the full menu of your pages
* **Canonical URLs** — Avoid duplicate content issues

---

## **`next/head` — Your Metadata Control Room**

### What is it?

`next/head` is a **Next.js component** that lets you put elements inside your HTML `<head>` tag directly from your React component.

Why is this important?
In SEO, the `<head>` section is **prime real estate** — that’s where search engines and social platforms look first.

---

### **Essential Metadata to Include**

1. **Title Tag**
   This is like your app’s name on the Play Store.
   *Groww’s stock page title*:
   `"Tata Motors Share Price Today - Tata Motors Ltd Stock Analysis | Groww"`
   It’s short, descriptive, and has keywords people actually search.

2. **Meta Description**
   Appears under your title in search results.
   Like a **Swiggy restaurant tagline**:
   `"Order delicious biryani from Paradise Restaurant in Hyderabad. Fast delivery, safe packaging."`

3. **OpenGraph Tags (OG)**
   Used by Facebook, WhatsApp, LinkedIn, etc., to generate link previews.
   Without them, your link looks boring.

4. **Twitter Cards**
   Like OG tags, but for Twitter/X — so your Groww IPO page looks good when shared.

5. **Canonical URLs**
   Your official, “this is the main version” link — prevents SEO penalties for duplicate content.

---

### **Example: Zomato-style Restaurant Page Metadata**

```jsx
import Head from 'next/head';

export default function RestaurantPage() {
  return (
    <>
      <Head>
        {/* Basic SEO */}
        <title>Paradise Biryani - Hyderabad | Order Online on CodingGita</title>
        <meta
          name="description"
          content="Order authentic Paradise Biryani from Hyderabad on CodingGita. Fast delivery, fresh ingredients, and unbeatable taste."
        />
        <meta name="keywords" content="biryani, Paradise Biryani, Hyderabad food, order online" />

        {/* OpenGraph for social media */}
        <meta property="og:title" content="Paradise Biryani - Hyderabad | Order Online" />
        <meta
          property="og:description"
          content="Taste Hyderabad's legendary Paradise Biryani, now on CodingGita. Order online and get it delivered hot."
        />
        <meta property="og:image" content="https://codinggita.com/images/paradise-biryani.jpg" />
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://codinggita.com/restaurants/paradise-biryani" />

        {/* Twitter Card */}
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="Paradise Biryani - Hyderabad" />
        <meta
          name="twitter:description"
          content="Order authentic Paradise Biryani from Hyderabad on CodingGita."
        />
        <meta name="twitter:image" content="https://codinggita.com/images/paradise-biryani.jpg" />

        {/* Canonical URL */}
        <link rel="canonical" href="https://codinggita.com/restaurants/paradise-biryani" />
      </Head>

      <h1>Paradise Biryani - Hyderabad</h1>
    </>
  );
}
```

---

## `robots.txt` & Its Limitations

* The `robots.txt` file lives at the **root** (e.g. `https://swiggy-clone.com/robots.txt`) and guides crawlers like Googlebot on *which parts of your site they may or may not crawl* ([Google for Developers][1], [Yoast][2]).
* It’s **advisory only**—respecting crawlers obey it; malicious bots often ignore it ([Wikipedia][3]).
* Blocking via `robots.txt` doesn’t guarantee de-indexing; URLs may still appear in search results without descriptions if linked elsewhere ([Google for Developers][1]).

---

## Example — What Should Be in `robots.txt`

Let’s imagine a Swiggy-like food delivery app. Here’s how you might structure your `robots.txt`, by features and rationale.

```
User-agent: *
Allow: /
```

* **`User-agent: *`** → applies to all crawlers.
* **`Allow: /`** → indicates general access is fine.

### 1. Block Non-Public / Functional Paths

These include APIs, admin, login, and order flows—none are meaningful for public indexing and may leak private data.

```
Disallow: /api/
Disallow: /admin/
Disallow: /auth/
Disallow: /login
Disallow: /logout
```

### 2. Prevent Duplication from Filters or Parameters

A food delivery app often has filter queries (e.g., `?cuisine=italian`) and category views. These create tons of similar URLs.

```
Disallow: /*?filter=
Disallow: /*?sort=
```

* Use wildcards (`*`) to block parameter-caused URL variants ([Prerender][4]).

### 3. Exclude Sensitive or Transactional Pages

Pages like the shopping cart, checkout, order tracking, and payment gateway aren’t useful for SEO and could expose sensitive flows.

```
Disallow: /cart
Disallow: /checkout
Disallow: /order-track
Disallow: /payment
```

### 4. E-commerce Best Practice Reminder

Do **not** use broad or “blanket” disallows that accidentally block important content — target only pages that should remain private or are low-value for search ([Prerender][4]).

### 5. Respect Crawlers’ Budget & Sitemap Tracking

Be explicit, add your sitemap(s) so crawlers can discover pages efficiently:

```
Sitemap: https://swiggy-clone.com/sitemap.xml
Sitemap: https://swiggy-clone.com/instamart/sitemap.xml.gz
```

Swiggy itself references multiple sitemaps; useful for segmented site areas (e.g., Instamart) ([Swiggy.com][5]).

---

## Swiggy’s Real `robots.txt` Example

Swiggy's actual `robots.txt` (as of now) looks like:

```
User-agent: *
Allow: /
Disallow: /product
Disallow: /product-category
Disallow: /api
Disallow: /dapi
Disallow: /mapi
Disallow: /?attachment_id=*
Disallow: /wp-admin
Disallow: /wp-includes
Disallow: /wp-content/plugins/
Disallow: /category/*
Disallow: /track-order/*
Disallow: /order-track/*
Disallow: /auth*
Disallow: /web-payments
Disallow: /invoice/*
Disallow: /home
Disallow: /cart
Disallow: /payment
Disallow: /checkout
Disallow: /my-account
Disallow: /rate
Sitemap: https://www.swiggy.com/sitemap.xml.gz
Sitemap: https://www.swiggy.com/instamart/sitemap/sitemap.xml.gz
```

Notice how they've blocked legacy WordPress paths (`/wp-admin` etc.), their APIs (`/api`, `/dapi`, `/mapi`), filters (`?attachment_id=*`), and private user flows like cart, checkout, order tracking, etc. They also provide multiple sitemaps ([Swiggy.com][5]).

---

## Advanced Features & Tips

| Directive                 | Purpose & Example                                                                                          |
| ------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `Crawl-delay`             | Throttle crawlers (supported by some like Bing/Yandex, ignored by Google) ([Wikipedia][6], [Wikipedia][3]) |
| `$` at end                | Match end-of-URL, e.g. `Disallow: /*.php$` blocks `.php` files only ([Wikipedia][6])                       |
| `Allow:` override         | Allow sub-paths inside disallowed paths (useful selectively) ([Conductor][7])                              |
| Comments (`#`)            | Annotate why rules exist—for future clarity ([seerinteractive.com][8])                                     |
| Multiple Sitemaps         | Helps structure large sites (e.g. separate Instamart section) ([SEOTesting.com][9])                        |
| Specific User-Agent rules | e.g. restrict just `Googlebot-Image`, allow `*` ([Google for Developers][10])                              |

---

## Recap

For a site like Swiggy:

1. **Allow general crawling** but **block private or sensitive pages**—they don't add SEO value and risk exposing user data.
2. **Avoid duplicate-content trap** by blocking parameter-heavy or filter-generated URLs.
3. **Provide sitemaps** for easier indexation and efficient crawling.
4. **Add notes/comments** in your `robots.txt` for clarity as your team evolves.
5. **Test your setup** via tools like Google Search Console's Robots Tester and validate syntax (one rule per line, proper order: User-agent → Disallow → Allow → Sitemap) ([Ignite Visibility][11]).

---

[1]: https://developers.google.com/search/docs/crawling-indexing/robots/intro?utm_source=codinggita.com "Robots.txt Introduction and Guide | Google Search Central"
[2]: https://yoast.com/ultimate-guide-robots-txt/?utm_source=codinggita.com "The ultimate guide to robots.txt - Yoast"
[3]: https://en.wikipedia.org/wiki/Robots.txt?utm_source=codinggita.com "Robots.txt"
[4]: https://prerender.io/blog/robots-txt-for-ecommerce-seo/?utm_source=codinggita.com "Robots.txt Best Practices for Ecommerce SEO - Prerender"
[5]: https://www.swiggy.com/robots.txt?utm_source=codinggita.com "robots.txt - Swiggy"
[6]: https://de.wikipedia.org/wiki/Robots_Exclusion_Standard?utm_source=codinggita.com "Robots Exclusion Standard"
[7]: https://www.conductor.com/academy/robotstxt/?utm_source=codinggita.com "Robots.txt for SEO: The Ultimate Guide - Conductor"
[8]: https://www.seerinteractive.com/insights/how-to-read-robots-txt?utm_source=codinggita.com "What is Robots.txt? A Guide for SEOs - Seer Interactive"
[9]: https://seotesting.com/google-search-console/robots-txt/?utm_source=codinggita.com "Robots.txt and SEO - The Ultimate Guide from the Experts"
[10]: https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt?utm_source=codinggita.com "Create and Submit a robots.txt File | Google Search Central"
[11]: https://ignitevisibility.com/the-newbies-guide-to-blocking-content-with-robots-txt/?utm_source=codinggita.com "Robots.txt Disallow: A Complete Guide - Ignite Visibility"

## **Sitemap.xml — Your Menu Card for Google**

A sitemap is like handing **Google a full restaurant menu** instead of making them guess dishes.

Place it at `/sitemap.xml`.
It lists **every important page** and tells search engines:

* When it was last updated
* How often it changes
* How important it is

**Example:**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://codinggita.com/</loc>
    <lastmod>2025-08-08</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
  </url>
  <url>
    <loc>https://codinggita.com/restaurants/paradise-biryani</loc>
    <lastmod>2025-08-08</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.9</priority>
  </url>
</urlset>
```

For **big apps** like Swiggy or Zomato:

* Generate **dynamic sitemaps** using API routes, pulling data from your DB.

---

## **Canonical URLs — Declaring the Official Page**

Imagine Zomato has:

* `zomato.com/paradise-biryani`
* `zomato.com/biryani/paradise`

Both show the same content. Without a **canonical tag**, Google thinks you’re duplicating content.

Add in `next/head`:

```jsx
<link rel="canonical" href="https://codinggita.com/restaurants/paradise-biryani" />
```

**Rules:**

* Always use **HTTPS** version.
* Point all duplicates to one **main page**.

---

## **Putting It All Together — Example: Groww Stock Page**

If we were building **Groww’s Tata Motors stock page** on CodingGita:

1. `next/head` → Title: `"Tata Motors Share Price Today | CodingGita Stocks"`, meta description, OG image of the company logo.
2. `robots.txt` → Allow public stock pages, block `/my-portfolio`.
3. `sitemap.xml` → Include this page with priority 0.9.
4. Canonical URL → Always point to `/stocks/tata-motors`.

---

## **Why Next.js Makes This Easier than React CRA**

In **React CRA**, metadata is injected **after JavaScript runs** — crawlers may miss it.
In **Next.js**, metadata is **server-rendered** — crawlers see it instantly.

This is why:

* Swiggy’s restaurant pages load with metadata ready.
* Groww’s stock pages have correct OG previews on social media.

---

✅ Create **a reusable SEO component** so you don’t repeat `<Head>` code.
✅ Use **absolute URLs** for OG images.
✅ Test your metadata using:

* Google’s Rich Results Test
* Facebook Sharing Debugger
* Twitter Card Validator

---

## **Conclusion**

When you combine:

1. **Accurate metadata (`next/head`)**
2. **Clear crawling rules (`robots.txt`)**
3. **Complete site maps (`sitemap.xml`)**
4. **Duplicate control (canonical URLs)**

… you’re not just building a site — you’re **opening your restaurant on the busiest street in town with bright lights, big signs, and beautiful menu boards.**

With **Next.js**, you get all the SEO benefits of SSR + the flexibility to control metadata per page.

---
